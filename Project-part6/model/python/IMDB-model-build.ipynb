{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMBA next purchase prediction\n",
        "\n",
        "## Using XGBoost in SageMaker\n",
        "\n",
        "---\n",
        "\n",
        "In this example of using Amazon's SageMaker service we will construct a random tree model to productionaze the predictive model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Preprocess the data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "bucket='imba'\n",
        "\n",
        "# we load a smaller version of the full dataset, you can increase the instance size to load the full dataset instead\n",
        "data_key = 'output_small/data_small.csv'\n",
        "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
        "\n",
        "data = pd.read_csv(data_location)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# load order product train dataframe\n",
        "order_product_train_key = 'data/order_products/order_products__train.csv.gz'\n",
        "order_product_train_location = 's3://{}/{}'.format(bucket, order_product_train_key)\n",
        "\n",
        "order_product_train = pd.read_csv(order_product_train_location)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# have a look at the data\n",
        "order_product_train.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>order_id</th>\n      <th>product_id</th>\n      <th>add_to_cart_order</th>\n      <th>reordered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>49302</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11109</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>10246</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>49683</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>43633</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   order_id  product_id  add_to_cart_order  reordered\n0         1       49302                  1          1\n1         1       11109                  2          1\n2         1       10246                  3          0\n3         1       49683                  4          0\n4         1       43633                  5          1"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# load orders dataframe\n",
        "orders_key = 'data/orders/orders.csv'\n",
        "orders_location = 's3://{}/{}'.format(bucket, orders_key)\n",
        "\n",
        "orders = pd.read_csv(orders_location)\n",
        "# only select train and test orders\n",
        "#orders = orders[orders.eval_set != 'prior'][['user_id', 'order_id','eval_set']]"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# attach user_id to order_product_train\n",
        "order_product_train = order_product_train.merge(orders[['user_id', 'order_id']])"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# attach eval_set to data\n",
        "#data = data.merge(orders[orders.eval_set != 'prior'][['user_id', 'order_id','eval_set']])\n",
        "data = data.merge(orders[orders.eval_set != 'prior'][['user_id','eval_set']])"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# attach target variable: reordered\n",
        "data = data.merge(order_product_train[['user_id', 'product_id', 'reordered']], how = 'left')"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# a few more feature engineering, refer to the R code\n",
        "data['prod_reorder_probability'] = data.prod_second_orders / data.prod_first_orders\n",
        "data['prod_reorder_times'] = 1 + data.prod_reorders / data.prod_first_orders\n",
        "data['prod_reorder_ratio'] = data.prod_reorders / data.prod_orders\n",
        "data.drop(['prod_reorders', 'prod_first_orders', 'prod_second_orders'], axis=1, inplace=True)\n",
        "data['user_average_basket'] = data.user_total_products / data.user_orders\n",
        "data['up_order_rate'] = data.up_orders / data.user_orders\n",
        "data['up_orders_since_last_order'] = data.user_orders - data.up_last_order\n",
        "data['up_order_rate_since_first_order'] = data.up_orders / (data.user_orders - data.up_first_order + 1)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>up_orders</th>\n      <th>user_mean_days_since_prior</th>\n      <th>user_period</th>\n      <th>user_distinct_products</th>\n      <th>user_reorder_ratio</th>\n      <th>user_total_products</th>\n      <th>up_average_cart_position</th>\n      <th>up_first_order</th>\n      <th>user_orders</th>\n      <th>...</th>\n      <th>user_id</th>\n      <th>eval_set</th>\n      <th>reordered</th>\n      <th>prod_reorder_probability</th>\n      <th>prod_reorder_times</th>\n      <th>prod_reorder_ratio</th>\n      <th>user_average_basket</th>\n      <th>up_order_rate</th>\n      <th>up_orders_since_last_order</th>\n      <th>up_order_rate_since_first_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19508</td>\n      <td>6</td>\n      <td>5.969574</td>\n      <td>2943</td>\n      <td>200</td>\n      <td>0.602434</td>\n      <td>497</td>\n      <td>7.833333</td>\n      <td>11</td>\n      <td>61</td>\n      <td>...</td>\n      <td>144185</td>\n      <td>test</td>\n      <td>NaN</td>\n      <td>0.348857</td>\n      <td>1.812378</td>\n      <td>0.448239</td>\n      <td>8.147541</td>\n      <td>0.098361</td>\n      <td>32</td>\n      <td>0.117647</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42307</td>\n      <td>1</td>\n      <td>5.969574</td>\n      <td>2943</td>\n      <td>200</td>\n      <td>0.602434</td>\n      <td>497</td>\n      <td>3.000000</td>\n      <td>55</td>\n      <td>61</td>\n      <td>...</td>\n      <td>144185</td>\n      <td>test</td>\n      <td>NaN</td>\n      <td>0.546166</td>\n      <td>3.424100</td>\n      <td>0.707952</td>\n      <td>8.147541</td>\n      <td>0.016393</td>\n      <td>6</td>\n      <td>0.142857</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35883</td>\n      <td>1</td>\n      <td>5.969574</td>\n      <td>2943</td>\n      <td>200</td>\n      <td>0.602434</td>\n      <td>497</td>\n      <td>8.000000</td>\n      <td>52</td>\n      <td>61</td>\n      <td>...</td>\n      <td>144185</td>\n      <td>test</td>\n      <td>NaN</td>\n      <td>0.357664</td>\n      <td>2.010219</td>\n      <td>0.502542</td>\n      <td>8.147541</td>\n      <td>0.016393</td>\n      <td>9</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13539</td>\n      <td>1</td>\n      <td>5.969574</td>\n      <td>2943</td>\n      <td>200</td>\n      <td>0.602434</td>\n      <td>497</td>\n      <td>12.000000</td>\n      <td>50</td>\n      <td>61</td>\n      <td>...</td>\n      <td>144185</td>\n      <td>test</td>\n      <td>NaN</td>\n      <td>0.176471</td>\n      <td>1.270588</td>\n      <td>0.212963</td>\n      <td>8.147541</td>\n      <td>0.016393</td>\n      <td>11</td>\n      <td>0.083333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>27966</td>\n      <td>3</td>\n      <td>5.969574</td>\n      <td>2943</td>\n      <td>200</td>\n      <td>0.602434</td>\n      <td>497</td>\n      <td>7.000000</td>\n      <td>53</td>\n      <td>61</td>\n      <td>...</td>\n      <td>144185</td>\n      <td>test</td>\n      <td>NaN</td>\n      <td>0.611129</td>\n      <td>4.330669</td>\n      <td>0.769089</td>\n      <td>8.147541</td>\n      <td>0.049180</td>\n      <td>0</td>\n      <td>0.333333</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>",
            "text/plain": "   product_id  up_orders  user_mean_days_since_prior  user_period  \\\n0       19508          6                    5.969574         2943   \n1       42307          1                    5.969574         2943   \n2       35883          1                    5.969574         2943   \n3       13539          1                    5.969574         2943   \n4       27966          3                    5.969574         2943   \n\n   user_distinct_products  user_reorder_ratio  user_total_products  \\\n0                     200            0.602434                  497   \n1                     200            0.602434                  497   \n2                     200            0.602434                  497   \n3                     200            0.602434                  497   \n4                     200            0.602434                  497   \n\n   up_average_cart_position  up_first_order  user_orders  ...  user_id  \\\n0                  7.833333              11           61  ...   144185   \n1                  3.000000              55           61  ...   144185   \n2                  8.000000              52           61  ...   144185   \n3                 12.000000              50           61  ...   144185   \n4                  7.000000              53           61  ...   144185   \n\n   eval_set  reordered prod_reorder_probability  prod_reorder_times  \\\n0      test        NaN                 0.348857            1.812378   \n1      test        NaN                 0.546166            3.424100   \n2      test        NaN                 0.357664            2.010219   \n3      test        NaN                 0.176471            1.270588   \n4      test        NaN                 0.611129            4.330669   \n\n   prod_reorder_ratio  user_average_basket  up_order_rate  \\\n0            0.448239             8.147541       0.098361   \n1            0.707952             8.147541       0.016393   \n2            0.502542             8.147541       0.016393   \n3            0.212963             8.147541       0.016393   \n4            0.769089             8.147541       0.049180   \n\n   up_orders_since_last_order  up_order_rate_since_first_order  \n0                          32                         0.117647  \n1                           6                         0.142857  \n2                           9                         0.100000  \n3                          11                         0.083333  \n4                           0                         0.333333  \n\n[5 rows x 22 columns]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# split into training and test set, test set does not have target variable\n",
        "train = data[data.eval_set == 'train'].copy()\n",
        "test = data[data.eval_set == 'test'].copy()"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# id field won't be used in model, thus make a backup of them and remove from dataframe\n",
        "#test_id = test[['product_id','user_id', 'order_id', 'eval_set']]\n",
        "test_id = test[['product_id','user_id', 'eval_set']]\n",
        "#test.drop(['product_id','user_id', 'order_id', 'eval_set', 'reordered'], axis=1, inplace=True)\n",
        "test.drop(['product_id','user_id', 'eval_set', 'reordered'], axis=1, inplace=True)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# convert target variable to 1/0 for training dataframe\n",
        "train['reordered'] = train['reordered'].fillna(0)\n",
        "train['reordered'] = train.reordered.astype(int)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# drop id columns as they won't be used in model\n",
        "#train.drop(['eval_set', 'user_id', 'product_id', 'order_id'], axis=1, inplace=True)\n",
        "train.drop(['eval_set', 'user_id', 'product_id'], axis=1, inplace=True)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the target variable dataframe\n",
        "train_y = train[['reordered']]\n",
        "# this is the dataframe without target variable\n",
        "train_X = train.drop(['reordered'], axis = 1)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Classification\n",
        "\n",
        "Now that we have created the feature representation of our training (and testing) data, it is time to start setting up and using the XGBoost classifier provided by SageMaker.\n",
        "\n",
        "### Writing the dataset\n",
        "\n",
        "The XGBoost classifier that we will be using requires the dataset to be written to a file and stored using Amazon S3. To do this, we will start by splitting the training dataset into two parts, the data we will train the model with and a validation set. Then, we will write those datasets to a file and upload the files to S3."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we split the data into training and validation set. Training data is for training the model, validation data is for evaluating the model performance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "val_X = train_X[:20000]\n",
        "train_X = train_X[20000:]\n",
        "\n",
        "val_y = train_y[:20000]\n",
        "train_y = train_y[20000:]\n",
        "\n",
        "#test_y = pd.DataFrame(test_y)\n",
        "#test_X = pd.DataFrame(test_X)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information about this and other algorithms, the SageMaker developer documentation can be found on __[Amazon's website.](https://docs.aws.amazon.com/sagemaker/latest/dg/)__"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# First we make sure that the local directory in which we'd like to store the training and validation csv files exists.\n",
        "import os\n",
        "data_dir = 'data/xgboost'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# First, save the test data to test.csv in the data_dir directory without label.\n",
        "pd.DataFrame(test).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
        "\n",
        "# Then we save the training and validation set into local disk as csv files\n",
        "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
        "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# To save a bit of memory we can set text_X, train_X, val_X, train_y and val_y to None.\n",
        "\n",
        "train_X = val_X = train_y = val_y = None"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading Training / Validation files to S3\n",
        "\n",
        "Amazon's S3 service allows us to store files that can be access by both the built-in training models such as the XGBoost model we will be using as well as custom models such as the one we will see a little later."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "\n",
        "session = sagemaker.Session() # Store the current SageMaker session\n",
        "\n",
        "# S3 prefix (which folder will we use)\n",
        "prefix = 'imba-xgboost'\n",
        "\n",
        "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
        "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
        "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a tuned XGBoost model\n",
        "\n",
        "Now that the data has been uploaded it is time to create the XGBoost model. The first step is to create an estimator object which will be used as the *base* of your hyperparameter tuning job."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker import get_execution_role\n",
        "\n",
        "# Our current execution role is require when creating the model as the training\n",
        "# and inference code will need to access the model artifacts.\n",
        "role = get_execution_role()"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
        "# As a matter of convenience, the training and inference code both use the same container.\n",
        "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
        "\n",
        "container = get_image_uri(session.boto_region_name, 'xgboost', '0.90-1')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The method get_image_uri has been renamed in sagemaker>=2.\n",
            "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
        "# As a matter of convenience, the training and inference code both use the same container.\n",
        "import sagemaker\n",
        "container = sagemaker.image_uris.retrieve('xgboost', session.boto_region_name, 'latest')\n",
        "#container = get_image_uri(session.boto_region_name, 'xgboost', '0.90-1')"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#       Create a SageMaker estimator using the container location determined in the previous cell.\n",
        "#       It is recommended that you use a single training instance of type ml.m4.xlarge. It is also\n",
        "#       recommended that you use 's3://{}/{}/output'.format(session.default_bucket(), prefix) as the\n",
        "#       output path.\n",
        "\n",
        "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
        "                                    role,                                    # What is our current IAM Role\n",
        "                                    instance_count=1,                  # How many compute instances\n",
        "                                    instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
        "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
        "                                    sagemaker_session=session)\n",
        "\n",
        "#       Set the XGBoost hyperparameters in the xgb object. Don't forget that in this case we have a binary\n",
        "#       label so we should be using the 'binary:logistic' objective.\n",
        "\n",
        "# Solution:\n",
        "xgb.set_hyperparameters(max_depth=5,\n",
        "                        eta=0.1,\n",
        "                        gamma=4,\n",
        "                        min_child_weight=6,\n",
        "                        subsample=0.8,\n",
        "                        silent=0,\n",
        "                        objective='binary:logistic',\n",
        "                        early_stopping_rounds=10,\n",
        "                        num_round=500)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the hyperparameter tuner\n",
        "\n",
        "Now that the base estimator has been set up we need to construct a hyperparameter tuner object which we will use to request SageMaker construct a hyperparameter tuning job.\n",
        "\n",
        "**Note:** If you don't want the hyperparameter tuning job to take too long, make sure to not set the total number of models (jobs) too high."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# First, make sure to import the relevant objects used to construct the tuner\n",
        "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
        "\n",
        "\n",
        "# create the tuner object:\n",
        "\n",
        "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb, # The estimator object to use as the basis for the training jobs.\n",
        "                                               objective_metric_name = 'validation:rmse', # The metric used to compare trained models.\n",
        "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
        "                                               max_jobs = 4, # The total number of models to train\n",
        "                                               max_parallel_jobs = 2, # The number of models to train in parallel\n",
        "                                               hyperparameter_ranges = {\n",
        "                                                    'max_depth': IntegerParameter(3, 12),\n",
        "                                                    'eta'      : ContinuousParameter(0.05, 0.5),\n",
        "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
        "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
        "                                                    'gamma': ContinuousParameter(0, 10),\n",
        "                                               })"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit the hyperparameter tuner\n",
        "\n",
        "Now that the hyperparameter tuner object has been constructed, it is time to fit the various models and find the best performing model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "s3_input_train = sagemaker.TrainingInput(s3_data=train_location, content_type='csv')\n",
        "s3_input_validation = sagemaker.TrainingInput(s3_data=val_location, content_type='csv')\n",
        "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".....................................................................................................................................!\n"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model\n",
        "\n",
        "Now that we've run our hyperparameter tuning job, it's time to see how well the best performing model actually performs. To do this we will use SageMaker's Batch Transform functionality. Batch Transform is a convenient way to perform inference on a large dataset in a way that is not realtime. That is, we don't necessarily need to use our model's results immediately and instead we can peform inference on a large number of samples. An example of this in industry might be peforming an end of month report. This method of inference can also be useful to us as it means to can perform inference on our entire test set. \n",
        "\n",
        "Remember that in order to create a transformer object to perform the batch transform job, we need a trained estimator object. We can do that using the `attach()` method, creating an estimator object which is attached to the best trained job."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# attach the model:\n",
        "\n",
        "xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2021-12-05 05:00:40 Starting - Preparing the instances for training\n",
            "2021-12-05 05:00:40 Downloading - Downloading input data\n",
            "2021-12-05 05:00:40 Training - Training image download completed. Training in progress.\n",
            "2021-12-05 05:00:40 Uploading - Uploading generated training model\n",
            "2021-12-05 05:00:40 Completed - Training job completed\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have an estimator object attached to the correct training job, we can proceed as we normally would and create a transformer object."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transformer object from the attached estimator. Using an instance count of 1 and an instance type of ml.m4.xlarge\n",
        "#       should be more than enough.:\n",
        "xgb_transformer = xgb_attached.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we actually perform the transform job. When doing so we need to make sure to specify the type of data we are sending so that it is serialized correctly in the background. In our case we are providing our model with csv data so we specify `text/csv`. Also, if the test data that we have provided is too large to process all at once then we need to specify how the data file should be split up. Since each line is a single entry in our data set we tell SageMaker that it can split the input on each line."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the transform job. Make sure to specify the content type and the split type of the test data.\n",
        "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".............................\u001b[34mArguments: serve\u001b[0m\n",
            "\u001b[34m[2021-12-05 05:11:37 +0000] [1] [INFO] Starting gunicorn 19.9.0\u001b[0m\n",
            "\u001b[34m[2021-12-05 05:11:37 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
            "\u001b[34m[2021-12-05 05:11:37 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
            "\u001b[34m[2021-12-05 05:11:37 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
            "\u001b[34m[2021-12-05 05:11:38 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
            "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)', 'urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)']. \n",
            "  monkey.patch_all(subprocess=True)\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:38:INFO] Model loaded successfully for worker : 21\u001b[0m\n",
            "\u001b[34m[2021-12-05 05:11:38 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
            "\u001b[34m[2021-12-05 05:11:38 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
            "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)', 'urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)']. \n",
            "  monkey.patch_all(subprocess=True)\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:38:INFO] Model loaded successfully for worker : 22\u001b[0m\n",
            "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)', 'urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)']. \n",
            "  monkey.patch_all(subprocess=True)\u001b[0m\n",
            "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)', 'urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)']. \n",
            "  monkey.patch_all(subprocess=True)\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:38:INFO] Model loaded successfully for worker : 23\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:38:INFO] Model loaded successfully for worker : 24\u001b[0m\n",
            "\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[32m2021-12-05T05:11:41.967:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:49:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[34m[2021-12-05:05:11:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:49:INFO] Sniff delimiter as ','\u001b[0m\n",
            "\u001b[35m[2021-12-05:05:11:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
          ]
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the transform job has executed and the result, the estimated sentiment of each review, has been saved on S3. Since we would rather work on this file locally we can perform a bit of notebook magic to copy the file to the `data_dir`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download: s3://sagemaker-ap-southeast-2-480972076311/xgboost-2021-12-05-05-06-49-605/test.csv.out to data/xgboost/test.csv.out\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/ec2-user/SageMaker\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "prediction=pd.read_csv('data/xgboost/test.csv.out', header=None, names=[\"prob\"])\n",
        "test_id = test_id.reset_index().drop(['index','eval_set'],axis = 1)\n",
        "test = test.reset_index().drop(['index'],axis = 1)\n",
        "pd.concat([test_id,test],axis=1).to_csv('data/xgboost/test_final.csv', index = False)\n",
        "!aws s3 cp 'data/xgboost/test_final.csv' 's3://imba/model_output/test_final.csv'"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}